# Hidden Markov Model

Imagine you're playing a guessing game with your friend whoâ€™s hiding behind a curtain. Every minute, they make a **sound** â€” sometimes a **bark**, sometimes a **meow**, or a **tweet**. You canâ€™t see them, but based on the sound, you **guess** whether theyâ€™re pretending to be a **dog**, **cat**, or **bird**.

- What you **hear**: sounds like "bark", "meow", etc. â†’ These are **observations**.
- What theyâ€™re **actually pretending to be** behind the curtain â†’ These are **hidden states**.
- They might switch from cat â†’ dog â†’ bird in some order.

You're trying to **predict** what theyâ€™re pretending to be (the hidden state) based on the **sequence of sounds (observations)** you hear.

Thatâ€™s the idea of a **Hidden Markov Model**!

---

## ðŸ’» **HMM: Technical Definition**

A **Hidden Markov Model (HMM)** is a **probabilistic model** used to model **sequences** where:

- You observe a sequence of events (observations),
- But the actual system that generated those observations (the hidden states) is **not directly visible**.

---

### ðŸ“Œ HMM Consists of:

1. **States**:  
   Hidden variables (e.g., `Rainy`, `Sunny`) â†’ these arenâ€™t observed directly.

2. **Observations**:  
   What you see (e.g., `Walk`, `Shop`, `Clean`).

3. **Transition Probabilities**:  
   Probability of moving from one state to another (e.g., `P(Rainy â†’ Sunny)`).

4. **Emission Probabilities**:  
   Probability of an observation given a state (e.g., `P(Walk | Sunny)`).

5. **Initial State Probabilities**:  
   Probability of starting in a particular state (e.g., `P(Sunny)`).

---

### ðŸ¤– What HMM Solves:

1. **Prediction (Evaluation)**:  
   Whatâ€™s the probability that a given sequence of observations occurred?  
   ðŸ› ï¸ Algorithm: **Forward Algorithm**

2. **Decoding**:  
   Whatâ€™s the most likely sequence of hidden states that led to these observations?  
   ðŸ› ï¸ Algorithm: **Viterbi Algorithm**

3. **Learning**:  
   Learn model parameters (transition/emission probs) from data.  
   ðŸ› ï¸ Algorithm: **Baum-Welch Algorithm (an Expectation-Maximization variant)**

---

### ðŸ“Š Simple Example:

#### States:  
â˜€ï¸ Sunny  
ðŸŒ§ï¸ Rainy  

#### Observations:  
ðŸ§ Walk, ðŸ›’ Shop, ðŸ§¹ Clean  

#### Emissions:
| State  | Walk | Shop | Clean |
|--------|------|------|-------|
| Sunny  | 0.6  | 0.3  | 0.1   |
| Rainy  | 0.1  | 0.4  | 0.5   |

#### Transitions:
| From \ To | Sunny | Rainy |
|-----------|--------|--------|
| Sunny     | 0.7    | 0.3    |
| Rainy     | 0.4    | 0.6    |

---

### âœ¨ Where is HMM Used?

- **Speech Recognition**
- **Part-of-Speech Tagging** in NLP
- **Bioinformatics** (e.g., gene prediction)
- **Activity Recognition**
- **Weather Forecasting**

---

## âœ… TL;DR:

> **HMM** is a statistical model where the system is assumed to be a **Markov process** with **hidden states**. You see the **effects** (observations), but try to figure out the **hidden causes** (states) behind them.


# Detail explanation of HMM



